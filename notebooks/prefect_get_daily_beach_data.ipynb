{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-retrieve daily beach data (Prefect)\n",
    "\n",
    "Automated daily job (in Prefect) that runs just after 7:30AM Sydney time when the pages are updated each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pendulum\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlite_utils import Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scaleway_s3_storage import connect_to_s3, dataframe_to_csv_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_url(url):\n",
    "    \"\"\"\n",
    "    Given a URL (string), retrieves html and\n",
    "    returns the html as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    html = requests.get(url)\n",
    "    if html.ok:\n",
    "        return html.text\n",
    "    else:\n",
    "        raise ValueError(\"{} could not be retrieved.\".format(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEACHMAPP_BASE_URL = \"https://www.environment.nsw.gov.au/beachmapp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEACHWATCH_FIELDS = {\n",
    "    \"navbar-title-text\" : \"Beach name\",\n",
    "    \"beach-timelapse-panel\": \"Data last updated\",\n",
    "    \"bw-status-text\": \"Pollution status\",\n",
    "    \"bw-air-temp-value\": \"Maximum forecast air temperature\",\n",
    "    \"bw-ocean-temp-value\": \"Water temperature\",\n",
    "    \"bw-weather-text\": \"Weather forecast\",\n",
    "    \"bw-swell\": \"Swell\",\n",
    "    \"bw-wind\": \"Wind\",\n",
    "    \"bw-patrol-info\": \"Patrol info\",\n",
    "    \"bw-rainfall\": \"Rainfall\",\n",
    "    \"bw-high-tide\": \"High tide\",\n",
    "    \"bw-low-tide\": \"Low tide\",\n",
    "    \"bw-alert-text\": \"Alert\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define various helper functions to parse the html pages to extract the data for each beach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beachwatch_data_for_class(beach_soup, classname, item_name):\n",
    "    \"\"\"\n",
    "    Given the parsed html from BeautifulSoup for a paticular page, \n",
    "    search for a specific class name within either a 'div' or 'span' block.\n",
    "    item_name is the user-friendly name that we have chosen for each class name\n",
    "    \"\"\"\n",
    "    if classname == \"bw-alert-text\":\n",
    "        item = beach_soup.find_all(\"div\", {\"class\": classname})\n",
    "        return ([x.contents[0] for x in item], item_name)\n",
    "    else:\n",
    "        item = beach_soup.find(\"div\", {\"class\": classname})\n",
    "        if item is None:\n",
    "            item = beach_soup.find(\"span\", {\"class\": classname})\n",
    "    return (item.contents[0], item_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data_for_beach(beach_soup, beachwatch_fields):\n",
    "    beach_data = []\n",
    "    for classname in beachwatch_fields:\n",
    "        try:\n",
    "            item, item_name = get_beachwatch_data_for_class(beach_soup, classname, beachwatch_fields[classname])\n",
    "        except:\n",
    "            item = classname\n",
    "            item_name = beachwatch_fields[classname]\n",
    "        beach_data.append((item, item_name))\n",
    "    return beach_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_beach_daily_data(beachmapp_html, beachwatch_fields):\n",
    "    \"\"\"\n",
    "    Given a string of html representing a Beachmapp page,\n",
    "    returns the daily data for that beach\n",
    "    \"\"\"\n",
    "\n",
    "    beach_soup = BeautifulSoup(beachmapp_html, \"html.parser\")\n",
    "\n",
    "    return get_all_data_for_beach(beach_soup, beachwatch_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_beach_list(base_url, main_html, url_path, bypass):\n",
    "    \"\"\"\n",
    "    Given the main page html, creates a list of the beach URLs from \n",
    "    this page which will be subsequently passed to scrape_beach_daily_data\n",
    "    \"\"\"\n",
    "\n",
    "    if bypass:\n",
    "        return [base_url]\n",
    "\n",
    "    main_page = BeautifulSoup(main_html, \"html.parser\")\n",
    "\n",
    "    return [\n",
    "        base_url.replace(\"/beachmapp\", \"\") + link.get(\"href\")\n",
    "        for link in main_page.find_all(\"a\")\n",
    "        if url_path in link.get(\"href\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_beaches_list(base_url, bypass):\n",
    "    base_html = retrieve_url(base_url)\n",
    "    region_URLs = create_beach_list(base_url, base_html, \"beachmapp/Beaches\", bypass)\n",
    "    all_beaches = []\n",
    "    for region_url in region_URLs:\n",
    "        region = region_url.split(\"/\")[-1]\n",
    "        region_html = retrieve_url(region_url)\n",
    "        beaches_list = create_beach_list(base_url, region_html, \"/beachmapp/Beach\", bypass)\n",
    "        all_beaches.append([region, beaches_list])\n",
    "    return [[region, item] for region, sublist in all_beaches for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_daily_beach_data_local(all_beach_daily_data_df, write_local=False):\n",
    "    if write_local is True:\n",
    "        data_filename = \"../data/all_beach_daily_data_\"\n",
    "        data_parquet = data_filename.replace(\"data_\", \"data.parquet\")\n",
    "        data_xlsx = data_filename + pendulum.now().isoformat() + \".xlsx\"\n",
    "        data_csv = data_xlsx.replace(\".xlsx\", \".csv\")\n",
    "        print(\"Created: \" + data_csv)\n",
    "        all_beach_daily_data_df.to_parquet(data_parquet)\n",
    "        all_beach_daily_data_df.to_excel(data_xlsx, index=False)\n",
    "        all_beach_daily_data_df.to_csv(data_csv, index=False)\n",
    "\n",
    "        db = Database(\"../data/daily_beach_data_db.sqlite\")\n",
    "        all_beach_daily_data_df.to_sql(\"beaches\", con=db.conn, if_exists=\"append\")\n",
    "    else:\n",
    "        bucket_name = \"databooth-beach-swim\"\n",
    "        data_filename = \"all_beach_daily_data.csv\"\n",
    "        s3 = connect_to_s3()\n",
    "        dataframe_to_csv_s3(s3, all_beach_daily_data_df, bucket_name, data_filename)\n",
    "        print(\"Wrote data to s3://\" + bucket_name + \"/\" + data_filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_beach_data(beachwatch_fields, beaches_url_list, write_local):\n",
    "    COLUMN_NAMES = [\"Retrieved at\"] + [\"Region\"] + list(beachwatch_fields.values())\n",
    "    all_beach_daily_data_df = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "\n",
    "    for region, beach_url in beaches_url_list:\n",
    "        beachmapp_html = retrieve_url(beach_url)\n",
    "        beach_data = scrape_beach_daily_data(beachmapp_html, beachwatch_fields)\n",
    "        scraped_time = pendulum.now().isoformat()\n",
    "        all_beach_daily_data_df.loc[len(all_beach_daily_data_df)] = [scraped_time] + [region] + [value for (value, _) in beach_data]\n",
    "\n",
    "    all_beach_daily_data_df[\"Alert\"] = all_beach_daily_data_df[\"Alert\"].apply(lambda alist: \" \".join(alist))\n",
    "\n",
    "    write_daily_beach_data_local(all_beach_daily_data_df, write_local)\n",
    "\n",
    "    return all_beach_daily_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data to s3://databooth-beach-swim/all_beach_daily_data.csv\n"
     ]
    }
   ],
   "source": [
    "# TODO: Could cache list of beaches (as generally not changing) and re-use\n",
    "\n",
    "WRITE_LOCAL = False\n",
    "\n",
    "\n",
    "beaches_url_list = create_all_beaches_list(BEACHMAPP_BASE_URL, False)\n",
    "all_beach_daily_data_df = get_daily_beach_data(BEACHWATCH_FIELDS, beaches_url_list, WRITE_LOCAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefect job considerations - necessary \"repackaging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beaches_url_list = create_all_beaches_list(BEACHMAPP_BASE_URL, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beach_urls = [beach_url for _, beach_url in beaches_url_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regions = [region for region, _ in beaches_url_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from prefect.schedules import Schedule\n",
    "from prefect.schedules.clocks import IntervalClock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_day_at_740am_sydney = Schedule(\n",
    "    clocks=[\n",
    "        IntervalClock(\n",
    "            interval=timedelta(days=1),\n",
    "            start_date=pendulum.datetime(\n",
    "                2022, 2, 26, 7, 40, 0, tz=\"Australia/Sydney\"),\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DateTime(2022, 3, 12, 7, 40, 0, tzinfo=Timezone('Australia/Sydney')),\n",
       " DateTime(2022, 3, 13, 7, 40, 0, tzinfo=Timezone('Australia/Sydney')),\n",
       " DateTime(2022, 3, 14, 7, 40, 0, tzinfo=Timezone('Australia/Sydney')),\n",
       " DateTime(2022, 3, 15, 7, 40, 0, tzinfo=Timezone('Australia/Sydney')),\n",
       " DateTime(2022, 3, 16, 7, 40, 0, tzinfo=Timezone('Australia/Sydney')),\n",
       " DateTime(2022, 3, 17, 7, 40, 0, tzinfo=Timezone('Australia/Sydney')),\n",
       " DateTime(2022, 3, 18, 7, 40, 0, tzinfo=Timezone('Australia/Sydney'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "every_day_at_740am_sydney.next(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_schedule = Schedule(\n",
    "    clocks=[\n",
    "        IntervalClock(\n",
    "            interval=timedelta(minutes=15),\n",
    "            start_date=pendulum.now(),\n",
    "            end_date=pendulum.now().add(minutes=46)\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DateTime(2022, 3, 11, 11, 34, 23, 880036, tzinfo=Timezone('Australia/Sydney')),\n",
       " DateTime(2022, 3, 11, 11, 49, 23, 880036, tzinfo=Timezone('Australia/Sydney')),\n",
       " DateTime(2022, 3, 11, 12, 4, 23, 880036, tzinfo=Timezone('Australia/Sydney'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_schedule.next(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc39c376ad17cc65fc87090bc7ebf4d4e3bb272db138912bbaab88a86af2553a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
